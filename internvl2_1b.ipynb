{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdraG6OOppSG",
        "outputId": "277647ec-4316-4206-9b17-74b7d51e088b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install git-lfs\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1h9q-z1p7T2",
        "outputId": "a84d222d-7dae-4593-b3b5-c4a61d3840e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (3.0.2-1ubuntu0.3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 19 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/\n",
        "!GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/OpenGVLab/InternVL2_5-1B\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AD0lyEc5qq2h",
        "outputId": "c351c619-449e-4f17-cb52-5cd5549ace33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n",
            "Cloning into 'InternVL2_5-1B'...\n",
            "remote: Enumerating objects: 74, done.\u001b[K\n",
            "remote: Counting objects: 100% (70/70), done.\u001b[K\n",
            "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
            "remote: Total 74 (delta 27), reused 0 (delta 0), pack-reused 4 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (74/74), 1.91 MiB | 1.39 MiB/s, done.\n",
            "fatal: cannot exec '/content/drive/MyDrive/InternVL2_5-1B/.git/hooks/post-checkout': Permission denied\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh /content/drive/MyDrive/InternVL2_5-1B\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94I-2XcJttIu",
        "outputId": "78ce2eb6-0112-4924-c653-365b579ac65f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 5.0M\n",
            "-rw------- 1 root root  790 Feb  8 11:48 added_tokens.json\n",
            "-rw------- 1 root root 3.7K Feb  8 11:48 config.json\n",
            "-rw------- 1 root root 5.5K Feb  8 11:48 configuration_intern_vit.py\n",
            "-rw------- 1 root root 3.8K Feb  8 11:48 configuration_internvl_chat.py\n",
            "-rw------- 1 root root   41 Feb  8 11:48 configuration.json\n",
            "-rw------- 1 root root  15K Feb  8 11:48 conversation.py\n",
            "drwx------ 2 root root 4.0K Feb  8 11:48 examples\n",
            "-rw------- 1 root root  129 Feb  8 11:48 generation_config.json\n",
            "-rw------- 1 root root 1.6M Feb  8 11:48 merges.txt\n",
            "-rw------- 1 root root  18K Feb  8 11:48 modeling_intern_vit.py\n",
            "-rw------- 1 root root  16K Feb  8 11:48 modeling_internvl_chat.py\n",
            "-rw------- 1 root root  135 Feb  8 11:48 model.safetensors\n",
            "-rw------- 1 root root  287 Feb  8 11:48 preprocessor_config.json\n",
            "-rw------- 1 root root  34K Feb  8 11:48 README.md\n",
            "drwx------ 3 root root 4.0K Feb  8 11:48 runs\n",
            "-rw------- 1 root root  744 Feb  8 11:48 special_tokens_map.json\n",
            "-rw------- 1 root root 8.9K Feb  8 11:48 tokenizer_config.json\n",
            "-rw------- 1 root root 3.3M Feb  8 11:48 vocab.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModel, AutoProcessor\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define model path\n",
        "path = \"OpenGVLab/InternVL2_5-1B\"\n",
        "local_dir = \"/content/drive/MyDrive/InternVL2_5-1B\"\n",
        "\n",
        "# Ensure device compatibility\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32  # Ensure model and input types match\n",
        "\n",
        "# Load the model\n",
        "model = AutoModel.from_pretrained(\n",
        "    path,\n",
        "    cache_dir=local_dir,\n",
        "    torch_dtype=torch_dtype,\n",
        "    low_cpu_mem_usage=True,\n",
        "    trust_remote_code=True\n",
        ").eval().to(device)\n",
        "\n",
        "# Load processor (acts as tokenizer)\n",
        "processor = AutoProcessor.from_pretrained(path, cache_dir=local_dir, trust_remote_code=True)\n",
        "\n",
        "# Function to preprocess image (Fix: Convert to torch.bfloat16 if using GPU)\n",
        "def preprocess_image(image_path, image_size=448):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = transform(image).unsqueeze(0).to(device)  # Add batch dimension & move to device\n",
        "    return image.to(torch_dtype)  # Ensure correct dtype for model\n",
        "\n",
        "# Load and preprocess the image\n",
        "image_path = \"/content/drive/MyDrive/P_DOC/HealthID.jpeg\"  # Replace with your actual image path\n",
        "image = preprocess_image(image_path)\n",
        "\n",
        "# Define text input\n",
        "text = \"Describe the image.\"\n",
        "\n",
        "# Fix: Use processor directly for text\n",
        "inputs = processor(text=[text], return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Fix: Ensure inputs are converted to model dtype\n",
        "inputs[\"pixel_values\"] = image\n",
        "\n",
        "# Run model inference\n",
        "with torch.no_grad():\n",
        "    response = model.chat(processor, inputs[\"pixel_values\"], text, generation_config={\"max_new_tokens\": 512})\n",
        "\n",
        "print(\"Assistant:\", response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_r66WbAb4gVj",
        "outputId": "7b2facb0-f51a-4709-c09f-616d8a75152e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assistant: The image shows a scanned copy of an Aadhaar card from the National Aadhaar Authority of India. The card includes the following details:\n",
            "\n",
            "- **Name:** Mahidumitha\n",
            "- **Aadhaar Number:** 91-226-2277-4311\n",
            "- **Full Name:** Mahidumitha\n",
            "- **Gender:** Female\n",
            "- **Date of Birth:** 05-12-2004\n",
            "- **Mobile Number:** 970598585\n",
            "- **QR Code:** Present on the card\n",
            "\n",
            "The card also includes a description of Aadhaar and its benefits, as well as a QR code for digital health services.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "from transformers import AutoModel, AutoProcessor\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "path = \"OpenGVLab/InternVL2_5-1B\"\n",
        "local_dir = \"/content/drive/MyDrive/InternVL2_5-1B\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "model = AutoModel.from_pretrained(\n",
        "    path,\n",
        "    cache_dir=local_dir,\n",
        "    torch_dtype=torch_dtype,\n",
        "    low_cpu_mem_usage=True,\n",
        "    trust_remote_code=True\n",
        ").eval().to(device)\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(path, cache_dir=local_dir, trust_remote_code=True)\n",
        "\n",
        "def preprocess_image(image_path, image_size=448):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    return transform(image).unsqueeze(0).to(device).to(torch_dtype)\n",
        "\n",
        "image_path = \"/content/sample.jpeg\"\n",
        "image = preprocess_image(image_path)\n",
        "\n",
        "text = \"You are a document ID API that extracts and sends details from driving licenses in JSON format . Extract the following details: Document Type, Issuing Authority, Name, Date of Birth, Gender, Address or Location (Location should be separate), Document Number, Expiry Date, Nationality, Other Identifiers.If the image is upside down,turned left or right ,  fix it and get the details. If the image is missing details please set the value as empty string like this  \\\"\\\" , If the image is not looks like ID means send an empty JSON. Only reply in JSON\"\n",
        "\n",
        "inputs = processor(text=[text], return_tensors=\"pt\").to(device)\n",
        "inputs[\"pixel_values\"] = image\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "with torch.no_grad():\n",
        "    response = model.chat(processor, inputs[\"pixel_values\"], text, generation_config={\"max_new_tokens\": 512})\n",
        "\n",
        "end_time = time.time()\n",
        "inference_time = end_time - start_time\n",
        "\n",
        "print(\"Assistant:\", response)\n",
        "print(f\"Inference Time: {inference_time:.4f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKNlupoF8kz8",
        "outputId": "1c952c80-8e63-44ae-d735-5adcdf6cdadc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assistant: ```json\n",
            "{\n",
            "  \"Document Type\": \"Driver's License\",\n",
            "  \"Issuing Authority\": \"Western Australia\",\n",
            "  \"Name\": \"MELSAUROU ROBERT PAUL\",\n",
            "  \"Date of Birth\": \"18 Apr 2027\",\n",
            "  \"Gender\": \"Male\",\n",
            "  \"Address\": \"ELLENROOK, WA 6060\",\n",
            "  \"Document Number\": \"LICENCE1024\",\n",
            "  \"Expiry Date\": \"07 Oct 1983\",\n",
            "  \"Nationality\": \"\",\n",
            "  \"Other Identifiers\": \"\"\n",
            "}\n",
            "```\n",
            "Inference Time: 9.0954 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "import torch\n",
        "import time\n",
        "from transformers import AutoModel, AutoProcessor\n",
        "from PIL import Image, ImageFile\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "path = \"OpenGVLab/InternVL2_5-1B\"\n",
        "local_dir = \"/content/drive/MyDrive/InternVL2_5-1B\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "model = AutoModel.from_pretrained(\n",
        "    path,\n",
        "    cache_dir=local_dir,\n",
        "    torch_dtype=torch_dtype,\n",
        "    low_cpu_mem_usage=True,\n",
        "    trust_remote_code=True\n",
        ").eval().to(device)\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(path, cache_dir=local_dir, trust_remote_code=True)\n",
        "\n",
        "def preprocess_image(image_path, image_size=448):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    try:\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "    except OSError:\n",
        "        print(f\"Error loading image: {image_path}. Check if the file is corrupted.\")\n",
        "        return None\n",
        "    return transform(image).unsqueeze(0).to(device).to(torch_dtype)\n",
        "\n",
        "image_path = \"/content/hariprem.jpg\"\n",
        "image = preprocess_image(image_path)\n",
        "\n",
        "if image is None:\n",
        "    exit()\n",
        "\n",
        "text = \"You are a document ID API that extracts and sends details from driving licenses in JSON format . Extract the following details: Document Type, Issuing Authority, Name, Date of Birth, Gender, Address or Location (Location should be separate), Document Number, Expiry Date, Nationality, Other Identifiers.If the image is upside down,turned left or right ,  fix it and get the details. If the image is missing details please set the value as empty string like this  \\\"\\\" , If the image is not looks like ID means send an empty JSON. Only reply in JSON\"\n",
        "\n",
        "inputs = processor(text=[text], return_tensors=\"pt\").to(device)\n",
        "inputs[\"pixel_values\"] = image\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "with torch.no_grad():\n",
        "    response = model.chat(processor, inputs[\"pixel_values\"], text, generation_config={\"max_new_tokens\": 512})\n",
        "\n",
        "end_time = time.time()\n",
        "inference_time = end_time - start_time\n",
        "\n",
        "print(\"Assistant:\", response)\n",
        "print(f\"Inference Time: {inference_time:.4f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8O0zHX8CENi",
        "outputId": "b5800bfb-1d30-4c70-af5f-9ec7f4f79394"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assistant: ```json\n",
            "{\n",
            "  \"Document Type\": \"Driving License\",\n",
            "  \"Issuing Authority\": \"TNI Malang\",\n",
            "  \"Name\": \"HARI PERC\",\n",
            "  \"Date of Birth\": \"27-05-1992\",\n",
            "  \"Gender\": \"\",\n",
            "  \"Address or Location\": \"\",\n",
            "  \"Document Number\": \"TAS123456789\",\n",
            "  \"Expiry Date\": \"\",\n",
            "  \"Nationality\": \"\",\n",
            "  \"Other Identifiers\": \"\"\n",
            "}\n",
            "```\n",
            "Inference Time: 4.3651 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "import torch\n",
        "import time\n",
        "from transformers import AutoModel, AutoProcessor\n",
        "from PIL import Image, ImageFile\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "path = \"OpenGVLab/InternVL2_5-1B\"\n",
        "local_dir = \"/content/drive/MyDrive/InternVL2_5-1B\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "model = AutoModel.from_pretrained(\n",
        "    path,\n",
        "    cache_dir=local_dir,\n",
        "    torch_dtype=torch_dtype,\n",
        "    low_cpu_mem_usage=True,\n",
        "    trust_remote_code=True\n",
        ").eval().to(device)\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(path, cache_dir=local_dir, trust_remote_code=True)\n",
        "\n",
        "def preprocess_image(image_path, image_size=448):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    try:\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "    except OSError:\n",
        "        print(f\"Error loading image: {image_path}. Check if the file is corrupted.\")\n",
        "        return None\n",
        "    return transform(image).unsqueeze(0).to(device).to(torch_dtype)\n",
        "\n",
        "image_path = \"/content/arif.JPG\"\n",
        "image = preprocess_image(image_path)\n",
        "\n",
        "if image is None:\n",
        "    exit()\n",
        "\n",
        "text = \"You are a document ID API that extracts and sends details from driving licenses in JSON format . Extract the following details: Document Type, Issuing Authority, Name, Date of Birth, Gender, Address or Location (Location should be separate), Document Number, Expiry Date, Nationality, Other Identifiers.If the image is upside down,turned left or right ,  fix it and get the details. If the image is missing details please set the value as empty string like this  \\\"\\\" , If the image is not looks like ID means send an empty JSON. Only reply in JSON\"\n",
        "\n",
        "inputs = processor(text=[text], return_tensors=\"pt\").to(device)\n",
        "inputs[\"pixel_values\"] = image\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "with torch.no_grad():\n",
        "    response = model.chat(processor, inputs[\"pixel_values\"], text, generation_config={\"max_new_tokens\": 512})\n",
        "\n",
        "end_time = time.time()\n",
        "inference_time = end_time - start_time\n",
        "\n",
        "print(\"Assistant:\", response)\n",
        "print(f\"Inference Time: {inference_time:.4f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5v1qqNk-J8i",
        "outputId": "c27c42f0-cd34-4187-fdf6-cec324c4e665"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assistant: ```json\n",
            "{\n",
            "  \"Document Type\": \"Driving License (Tamil Nadu)\",\n",
            "  \"Issuing Authority\": \"TNSS\",\n",
            "  \"Name\": \"Abdullah Aboodherer\",\n",
            "  \"Date of Birth\": \"19-11-1980\",\n",
            "  \"Gender\": \"\",\n",
            "  \"Address or Location\": \"\",\n",
            "  \"Document Number\": \"TNSS20200000000000004\",\n",
            "  \"Expiry Date\": \"\",\n",
            "  \"Nationality\": \"\",\n",
            "  \"Other Identifiers\": \"\"\n",
            "}\n",
            "```\n",
            "Inference Time: 9.5662 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "import torch\n",
        "import time\n",
        "from transformers import AutoModel, AutoProcessor\n",
        "from PIL import Image, ImageFile\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "path = \"OpenGVLab/InternVL2_5-1B\"\n",
        "local_dir = \"/content/drive/MyDrive/InternVL2_5-1B\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "model = AutoModel.from_pretrained(\n",
        "    path,\n",
        "    cache_dir=local_dir,\n",
        "    torch_dtype=torch_dtype,\n",
        "    low_cpu_mem_usage=True,\n",
        "    trust_remote_code=True\n",
        ").eval().to(device)\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(path, cache_dir=local_dir, trust_remote_code=True)\n",
        "\n",
        "def preprocess_image(image_path, image_size=448):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    try:\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "    except OSError:\n",
        "        print(f\"Error loading image: {image_path}. Check if the file is corrupted.\")\n",
        "        return None\n",
        "    return transform(image).unsqueeze(0).to(device).to(torch_dtype)\n",
        "\n",
        "image_path = \"/content/mukundan.jpeg\"\n",
        "image = preprocess_image(image_path)\n",
        "\n",
        "if image is None:\n",
        "    exit()\n",
        "\n",
        "text = \"You are a document ID API that extracts and sends details from driving licenses in JSON format . Extract the following details: Document Type, Issuing Authority, Name, Date of Birth, Gender, Address or Location (Location should be separate), Document Number, Expiry Date, Nationality, Other Identifiers.If the image is upside down,turned left or right ,  fix it and get the details. If the image is missing details please set the value as empty string like this  \\\"\\\" , If the image is not looks like ID means send an empty JSON. Only reply in JSON\"\n",
        "\n",
        "inputs = processor(text=[text], return_tensors=\"pt\").to(device)\n",
        "inputs[\"pixel_values\"] = image\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "with torch.no_grad():\n",
        "    response = model.chat(processor, inputs[\"pixel_values\"], text, generation_config={\"max_new_tokens\": 512})\n",
        "\n",
        "end_time = time.time()\n",
        "inference_time = end_time - start_time\n",
        "\n",
        "print(\"Assistant:\", response)\n",
        "print(f\"Inference Time: {inference_time:.4f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qs8SDqM1ClMG",
        "outputId": "c2653412-fcd6-49f6-f3ae-94077df5e904"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assistant: ```json\n",
            "{\n",
            "  \"Document Type\": \"Union of India Driving Licence (Tamil Nadu)\",\n",
            "  \"Issuing Authority\": \"Union of India\",\n",
            "  \"Name\": \"MUKUNTHAN KALAISELVAN\",\n",
            "  \"Date of Birth\": \"02-05-2001\",\n",
            "  \"Gender\": \"O+\",\n",
            "  \"Address or Location\": \"\",\n",
            "  \"Document Number\": \"TN85 20200000198\",\n",
            "  \"Expiry Date\": \"01-05-2041\",\n",
            "  \"Nationality\": \"\",\n",
            "  \"Other Identifiers\": \"\"\n",
            "}\n",
            "```\n",
            "Inference Time: 8.3844 seconds\n"
          ]
        }
      ]
    }
  ]
}